{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9TskFNFeX3S"
   },
   "source": [
    "# Week 2. Descriptive Statistics and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8GZJzU6eelM"
   },
   "source": [
    "In urban data science, often the first thing we want to do is get an understanding of what the data looks like before we dive in too much. We do this by investigating the \"descriptive statistics\" of the data.\n",
    "\n",
    "We might ask questions like:\n",
    "\n",
    "What is the minimum value? What is the maximum value? Mean? Standard deviation? Is there a relationship between two variables?\n",
    "\n",
    "Once we start looking at the data, we might find it is messy. This means there might be missing values or errors. There might be a lot of noise that we need to exclude in order to get to the signal we are looking for.\n",
    "\n",
    "In this class, we will walk through how to explore descriptive stats for a new data set, how to identify \"bad data\", and how to clean your data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iV-xkR39eV8D"
   },
   "outputs": [],
   "source": [
    "# Always start with your imports, we will need pandas, numpy, and matplotlib today\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOS_HUd_h1tY"
   },
   "source": [
    "We will work with a file of census information from LA County today. Go into this week's folder on Canvas and download the file \"lacounty_tracts.csv\".\n",
    "\n",
    "If you save it in the same folder as this notebook, you can load it simply as:\n",
    "\n",
    "`pd.read_csv('lacounty_tracts.csv')`\n",
    "\n",
    "This is called a **relative path** meaning it is relative to this notebook.\n",
    "\n",
    "You can also always reference a file by its **absolute path**. This is the path it lives at always, irrespective to this notebook.\n",
    "\n",
    "An absolute path looks like this:\n",
    "\"/Users/madilore/Documents/IUS/Week2/lacounty_tracts.csv\"\n",
    "\n",
    "You can find the path of your folder by going into your files/finder and right-clicking on the folder. You should see something that says \"New Terminal at Folder\". This will open up a Terminal/Command Prompt. Then, type `pwd` and hit `Enter`. This means \"print working directory\". This is the full file path to your folder.\n",
    "\n",
    "Note: Windows users. Your file path might have slashes that go the other way \"\\\\\". In python, regardless of your operating system, the file path will have \"/\" forward slashes separating the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUmCijEAhso3"
   },
   "outputs": [],
   "source": [
    "# Load the data file\n",
    "df = pd.read_csv('lacounty_tracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCcf4AvGjiQo"
   },
   "outputs": [],
   "source": [
    "#Let's look at what is inside the file\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2OxiYPhjnDV"
   },
   "source": [
    "The column names are:\n",
    "\n",
    "*   `GEOID` the identifier for the census tract\n",
    "*   `tot_pop`: Total population\n",
    "*   `per_white`: The percent of people identifying as non-Hispanic white\n",
    "*   `per_black`: The percent of people identifying as African American or non-Hispanic black\n",
    "*   `per_asian`: The percent of people identifying as Asian\n",
    "*   `per_hispanic`: The percent of people identifying as Hispanic\n",
    "*   `med_income`: The median household income\n",
    "*   `per_bach`: The percent of people with a bachelor's degree or higher\n",
    "*   `per_age2534`: The percent of people aged 25-34\n",
    "*   `per_age65up`: The percent of people aged 65 or older\n",
    "*   `per_female`: The percent of people identifying as female\n",
    "*   `h_value`: Average home value\n",
    "*   `singleh`: The number of single detached units\n",
    "*   `vacant`: The total number of vacant housing units\n",
    "*   `hunit`: The total number of housing units\n",
    "*   `dist_univ`: The distance from the center of the tract to the nearest university\n",
    "*   `dist_center`: The distance from the center of the tract to the center of the city (LA)\n",
    "*   `area`: The area of the census tract in sq km\n",
    "*   `per_age2024`: The percent of people aged 20-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIz9XEwpjlLN"
   },
   "outputs": [],
   "source": [
    "# We can start by getting descriptive stats of all the numeric variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBXAmEEi2JzO"
   },
   "outputs": [],
   "source": [
    "# We can also see the variables plotted out to see distributions\n",
    "plt.hist(df.tot_pop, bins=20) # This is a histogram with 20 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6zEfv8_dvLv"
   },
   "outputs": [],
   "source": [
    "# We can also build a histogram in pandas\n",
    "df.tot_pop.plot(kind='hist', bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZmSAqpvd5SO"
   },
   "source": [
    "A powerful feature in pandas is called `groupby`. This allows you to group the data by values that are the same within a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJo6Ah1seEWR"
   },
   "outputs": [],
   "source": [
    "# For example, let's group the data by majority white (True, False)\n",
    "# And then describe the percent of single detached units\n",
    "\n",
    "df.groupby('maj_white')['singleh'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe5UpZPS3-DD"
   },
   "source": [
    "Now, recall important metrics of central tendancy:\n",
    "\n",
    "\n",
    "* Mean (`df[column].mean()`): the average of all values\n",
    "* Median (`df[column].median()`): the “middle” value if you order them, ie. at the 50th percentile\n",
    "* Mode (`df[column].mode()`): the most common value\n",
    "* Count (`df[column].count()`): the total number of values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9C_r4gR2pO0"
   },
   "outputs": [],
   "source": [
    "column = 'tot_pop'\n",
    "print(f'The mean of {column} is {df[column].mean():.2f}')\n",
    "print(f'The median of {column} is {df[column].median()}')\n",
    "print(f'The mode of {column} is {df[column].mode()}')\n",
    "print(f'The count of {column} is {df[column].count()}')\n",
    "\n",
    "#What is happening with the mode?\n",
    "df[column].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUFZCAsp7o-2"
   },
   "source": [
    "## Data cleaning\n",
    "\n",
    "Most of the time the data we collect is messy. We can go through a number of steps to clean it up in order to prepare the data for efficient and accurate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZMdWVaK4Tyd"
   },
   "outputs": [],
   "source": [
    "# Let's look at our data again\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK9JAcAlEXu4"
   },
   "source": [
    "What are some weird things we notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFEDEoNy5N9-"
   },
   "outputs": [],
   "source": [
    "df[df['med_income'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukCthklhEoXE"
   },
   "outputs": [],
   "source": [
    "# NaN means \"not a number\". It represents a null or empty space\n",
    "# It looks like some rows had a default of -666666666 set for missing values\n",
    "# Let's set these to NaNs so we don't include them in analysis (mean, median, etc)\n",
    "\n",
    "# We can replace it directly for all of these. Remember .loc pulls specific rows\n",
    "# The rows we want are the ones that return True for:\n",
    "df[df['med_income'] < 0]\n",
    "\n",
    "df.loc[df['med_income'] < 0, 'med_income'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWKTXSjtFdwi"
   },
   "outputs": [],
   "source": [
    "# Let's look again\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kizk91tjGfn8"
   },
   "outputs": [],
   "source": [
    "# In fact, none of our variables should go below zero\n",
    "# Note: only do an operation like this if you are certain you want it!\n",
    "df[df < 0] = np.nan\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-DKYD3VFm-g"
   },
   "outputs": [],
   "source": [
    "# It looks the counts are all over the place!\n",
    "# How long is our dataframe in total?\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1JaOSQ5Fz8h"
   },
   "outputs": [],
   "source": [
    "# Look at df.describe() before and after we set the negative med_incomes to NaN\n",
    "# NaNs are not included in count because they are \"empty\" cells\n",
    "df[df['med_income'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edDpzzSdGJFd"
   },
   "outputs": [],
   "source": [
    "# Sometimes only the median income is missing, but sometimes the whole row is missing\n",
    "# We can see how many NaNs exist in each row\n",
    "df.isna().sum(axis=1) #axis=1 means compute by row; axis=0 means by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulx9MYG2Td8a"
   },
   "outputs": [],
   "source": [
    "# We can see how many of each number there is\n",
    "df.isna().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7x0KqU27Tu7j"
   },
   "outputs": [],
   "source": [
    "# Maybe we can tolerate 1 or 2 missing/NaN values but no more than that\n",
    "# Then, we want to exclude these rows\n",
    "\n",
    "# Best practice is to make a new dataframe so you always have an original\n",
    "df_clean = df[df.isna().sum(axis=1) <= 2]\n",
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKilyM4rUlU3"
   },
   "source": [
    "Everything is starting to look better and more clean!\n",
    "It looks like our two trouble variables are `med_income` and `h_value`.\n",
    "\n",
    "There are a few options:\n",
    "\n",
    "1. Sometimes we are happy just leaving these as is and knowing there are some missing values. In this case, we consider the data cleaned! This might be the case when we want to get to overall patterns and metrics and small variation is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQMuCHXSUkzM"
   },
   "outputs": [],
   "source": [
    "# This is our new dataset!\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv37byYLWIVT"
   },
   "source": [
    "2. Othertimes, we want to exclude these variables alltogether because they are incomplete. This might be the case if we had a lot of missing values. At this point, there might be too much missing for us to consider it an accurate picture of our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hXVm4HjUHkl"
   },
   "outputs": [],
   "source": [
    "df_noincome = df.drop('med_income', axis=1) #we use axis=1 because we want to do this for every row\n",
    "df_noincome.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3wriRW7WeF5"
   },
   "source": [
    "3. We might also just exclude the rows that are missing these few variables. We are missing 12 rows of median income, so we might decide to exclude these rows from our analysis all together. However, we are missing 116 home values. This may be too many rows to exclude depending on what we care about. 116/2469=0.047, or 4.7% of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alfcdHqzWWpe"
   },
   "outputs": [],
   "source": [
    "df_excrows = df.loc[~df['h_value'].isna(), :]\n",
    "print(len(df_excrows))\n",
    "df_excrows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvL0YtMPWgf9"
   },
   "source": [
    "\n",
    "4. Finally, we might try to estimate the missing values based on the rest of the data. We might do this if we don't have too many missing values, we think this variable is important, but estimating the value will not impact the results too much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnltTBZ4Wg3f"
   },
   "outputs": [],
   "source": [
    "# We might set the missing values to the average\n",
    "df_newvalues = df.copy()\n",
    "df_newvalues['med_income'] = df_newvalues['med_income'].fillna(df['med_income'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLBw9U3aXBb1"
   },
   "outputs": [],
   "source": [
    "# Or the median\n",
    "df_newvalues = df.copy()\n",
    "df_newvalues['med_income'] = df_newvalues['med_income'].fillna(df['med_income'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DMGHsFWXGlV"
   },
   "outputs": [],
   "source": [
    "df_newvalues.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJPY1JQwe8oz"
   },
   "outputs": [],
   "source": [
    "## Now let's look at some comparisons with our clean data\n",
    "# Let's groupby majority white (True, False) and look at a few variables\n",
    "df_clean.groupby('maj_white')['h_value'].plot(kind='hist', alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVmJExT7ay_s"
   },
   "source": [
    "## Your Turn\n",
    "\n",
    "Let's put together everything we've learned over the past two weeks with a new dataset.\n",
    "\n",
    "Download `newyork_housing.csv` from Canvas and save it in the same folder as this notebook. Or save it in a data folder and replace the read file path below with the absolute path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozt5-DLtXI7F"
   },
   "outputs": [],
   "source": [
    "ny = pd.read_csv('newyork_housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPh-zyOXbFcq"
   },
   "outputs": [],
   "source": [
    "## YOUR TURN\n",
    "## Invesigate the columns in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMHZtocNbJe-"
   },
   "outputs": [],
   "source": [
    "## YOUR TURN\n",
    "## Clean up any data you think it messy, an error, or missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylPfX_Q5bNha"
   },
   "outputs": [],
   "source": [
    "## YOUR TURN\n",
    "## Compute some descriptive stats. What might be interesting about this data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKv4Dz_PbTKm"
   },
   "outputs": [],
   "source": [
    "## YOUR TURN\n",
    "## Plot some of the variables either alone or against one another.\n",
    "## Consider grouping some of the variables first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PSCId8-fcbH"
   },
   "outputs": [],
   "source": [
    "## YOUR TURN\n",
    "## Write a function to compute a new variable and iterate over the values in a column\n",
    "\n",
    "## Set it as a new column\n",
    "## Pro tip: df['new_column_name'] = df['input_variable'].apply(lambda x: function_name(x))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM5dru8UcD7hwyfdp/tVd4l",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
