{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5d9deb-866e-4773-832f-64dc4209f2df",
   "metadata": {},
   "source": [
    "# Week 4: APIs and Distance Metrics\n",
    "\n",
    "Today we will continue learning about gathering data. We discussed open data portals and how to get US Census data from the Census Bureau API. We will continue learning about different API types and learn how to use the Google Maps API to geolocate places, get directions, and more. Through this, we will also learn about different distance measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121030e-1ea1-477f-8600-c54f60815dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4b635-f37d-4a23-9798-5929c37bad52",
   "metadata": {},
   "source": [
    "### Brief aside: dealing with strings\n",
    "\n",
    "Recall: strings are sets of text made up of individual characters. There are a few important ways you can manipulate strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c638a0e-2a10-467b-bc19-328b7a242af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that strings can be indexed just like lists\n",
    "address = \"921 University Ave, Ithaca, NY 14853\"\n",
    "address[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7404f-26c2-48cc-a9d9-6acb6f021fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also replace chunks of text with another\n",
    "# The format is x.replace(old_text, new_text)\n",
    "address.replace(\"University\", \"College\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74146696-74c0-40d3-b35a-9a783fa53800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is helpful when you want to remove characters, you can replace them with empty strings: \"\"\n",
    "address.replace(\"921\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60eb90c-485f-432f-8be5-7609fe7430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will replace ALL instances of the old text, like for example spaces:\n",
    "address.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adface3-e1cf-48bc-8c5b-0d70aa06c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also concatenate or join text in a list together into one string:\n",
    "# The format is \"separator\".join(list)\n",
    "\n",
    "address_chunks = [\"921\", \"University\", \"Ave\", \"Ithaca\", \"NY\", \"14853\"]\n",
    "# You usually want to add a space between words so the separator can be \" \"\n",
    "\" \".join(address_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b218f1f-a281-4d59-9a14-63966a99df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But it can be any character that you want\n",
    "\"*\".join(address_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf3de1-2136-4753-bddd-51d5bd14c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our list of new york housing listings\n",
    "\n",
    "nyc_housing = pd.read_csv('newyork_housing.csv')\n",
    "nyc_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6409de5-3008-48d7-a1cf-9edfeb528403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With dataframes, you can also add text a few different ways:\n",
    "\n",
    "#For example lets save the city state\n",
    "nyc_housing['citystate'] = nyc_housing['address/city'] + \", \" + nyc_housing['address/state']\n",
    "nyc_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b5a7c-dd58-40de-bca9-161f5712f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use .apply and .join\n",
    "\n",
    "nyc_housing['citystate'] = nyc_housing.apply(lambda x: \", \".join([x['address/city'], x['address/state']]), axis=1) \n",
    "# We add axis=1 when we .apply to more than one column, in this case the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691308a-3bf8-4759-a548-16e88f4a12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error! .join expects strings and it looks like sometimes we have NaN values\n",
    "# We can REPLACE the nans with empty strings and then ensure the type is a string\n",
    "\n",
    "nyc_housing['address/city'] = nyc_housing['address/city'].astype(str).replace(np.nan, '')\n",
    "nyc_housing['address/state'] = nyc_housing['address/state'].astype(str).replace(np.nan, '')\n",
    "nyc_housing['citystate'] = nyc_housing.apply(lambda x: \", \".join([x['address/city'], x['address/state']]), axis=1) \n",
    "nyc_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52617b10-3a08-4e39-8a5c-94890f2fac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR TURN\n",
    "## Create a new column that has the whole string address \n",
    "\n",
    "## HINT You will want to make the zipcodes into integers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839bcc1-423d-4240-96a9-654923a9dc10",
   "metadata": {},
   "source": [
    "## Google Maps API\n",
    "\n",
    "One powerful API we can use is from Google Maps.\n",
    "\n",
    "First, we need to get set up in the Google Cloud Console. \n",
    "\n",
    "Once we are set up, we can import the `googlemaps` package and insert our API key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86070776-e2d6-4f60-baa6-6b6224114493",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebd424-af5e-4cfc-bc39-4b206cca69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "from gmaps_key import API_KEY # Add your key to the gmaps_key.py file\n",
    "\n",
    "# Just like with the census data, we need to provide our API_KEY to the googlemaps library.\n",
    "gmaps = googlemaps.Client(key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22bd8c-644b-4422-8e94-38661668485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the latitude-longitude location of an address\n",
    "\n",
    "geocode_result = gmaps.geocode(\"921 University Ave, Ithaca, NY 14853\")\n",
    "geocode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60a159-b77b-4c9b-8cc1-baafcfd51c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a list of length = 1 because we gave it one address\n",
    "# Inside is a dictionary\n",
    "\n",
    "# We can get just the location we need:\n",
    "geocode_result[0]['geometry']['location'] # It is really nested in there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8d8a8-e353-4b94-b8c9-92bed7b106b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply the API to a set of addresses from a dataframe:\n",
    "# Looping through each row of a dataframe uses .iterrows() like this\n",
    "\n",
    "# First let's create a temporary smaller dataframe to make this quicker\n",
    "tmp = nyc_housing.iloc[0:100]\n",
    "\n",
    "# Now, let's save the lat and lon of each address\n",
    "all_lats, all_lons = [],[]\n",
    "for idx, row in tmp.iterrows():\n",
    "    location = gmaps.geocode(row['full_address'])\n",
    "    lat = location[0]['geometry']['location']['lat']\n",
    "    lon = location[0]['geometry']['location']['lng']\n",
    "    all_lats.append(lat)\n",
    "    all_lons.append(lon)\n",
    "\n",
    "tmp['geocode_lat'] = all_lats\n",
    "tmp['geocode_lon'] = all_lons\n",
    "tmp.head()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01a855-db71-4578-ba3d-b9815b6c9bc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can also go the opposite direction. Take the lat/lon values and get an address.\n",
    "\n",
    "house = tmp.iloc[0][['full_address', 'geocode_lat', 'geocode_lon']]\n",
    "## Note, that the format here is Lat, Lng! (y, x)\n",
    "reverse_geocode_result = gmaps.reverse_geocode((house['geocode_lat'],house['geocode_lon']))\n",
    "reverse_geocode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e562c-cd20-4a76-88ed-102f84d99a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reverse_geocode_result[1]['formatted_address'])\n",
    "print(house['full_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0a415-b559-4e10-975b-a980eed1fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR TURN \n",
    "## Compute the lat/lon coordinates of a set of listings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e3330-98c0-4024-9d5b-274c9ff35f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in our citibike stations and compute the distance between them and the listings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984709d-224b-4da7-93ef-2ada164c45c0",
   "metadata": {},
   "source": [
    "## (Optional) Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72786f4-cea9-48bd-9488-f485e4338a9d",
   "metadata": {},
   "source": [
    "Webscraping is a method of programmatically retrieving data from websites. It is used when there are no APIs or when data that is published from the website isn't made avaliable in a downloadable format. \n",
    "\n",
    "#### A note on webscrapping:\n",
    "Webscrapping can be used for all kinds of malicious purposes, for instance, to copy website content and republish it. Here's a [complaint from Craiglist](https://www.scribd.com/doc/313832868/CraigslistVRadpad-Complaint?secret_password=7gTybamKvrbeVhxfi4mx) about a company called Radpad scraping Craigslist and reposting those listing on their own website:\n",
    "\n",
    "<mark>\n",
    "“[The content scraping service] would, on a daily basis, send an army of digital robots to craigslist to copy and download the full text of millions of craigslist user ads. [The service] then indiscriminately made those misappropriated listings available—through its so-called ‘data feed’—to any company that wanted to use them, for any purpose. Some such ‘customers’ paid as much as $20,000 per month for that content…”</mark>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<mark>\n",
    "According to the claim, scraped data was used for spam and email fraud, among other activities: </mark>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<mark>\n",
    "“[The defendants] then harvest craigslist users’ contact information from that database, and initiate many thousands of electronic mail messages per day to the addresses harvested from craigslist servers…. [The messages] contain misleading subject lines and content in the body of the spam messages, designed to trick craigslist users into switching from using craigslist’s services to using [the defenders’] service…”\n",
    "</mark>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Uff. \n",
    "\n",
    "**What about webscrapping for research or academic purposes?** Most of the above issues most likely won't apply to you, but webscrapping makes a website's traffic *spike* if you don't modulate how often you're pinging the website. This can cause the website's server to crash. This is not very nice. Also, a lot of websites won't allow you to do it. (If you go to almost any URL and put `/robots.txt` after it, you can see a list of subdomains that site will or won't allow you to scrape.)\n",
    "\n",
    "(adapted from Wenfei Xu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c9223-aabf-42c9-bd35-9dd5454c36c1",
   "metadata": {},
   "source": [
    "However, if you webscrape ethically and legally, in Python the place to start is with `beautifulsoup`. Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac90a0-023a-4f1f-a64d-09fa877e93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ba2e6-ca9f-46f0-8bdd-e103023979e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa66d4-0427-4d7c-8c1f-c3c3874b4e34",
   "metadata": {},
   "source": [
    "Here, we are importing BeautifulSoup from the bs4 library, which we will use to parse the HTML data we scrape from a website. We are also importing `requests`, which we will use to send HTTP requests to the website and retrieve the HTML data.\n",
    "\n",
    "The next step is to send a request to the website and retrieve the HTML data. You can do this using the `requests.get()` method, which takes the website URL as a parameter and returns the HTML content of the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e92ab-6b09-496d-85e9-4da5667a3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a list of Los Angeles houses available for purchase\n",
    "url = 'https://www.mlslistings.com/Search/Result/los-angeles/1?criteria=H4sIAAAAAAAACnWPwU7DMBBEfwXtFRttHNdpfEOFAzekph-wcbbBUuRU9ga1Qvw7CoQDB06jndHb0XxAYcrh7VQ4H3niIHFOLwN4VFuyHlD1ja1DOGtHfaWtdajbintd01Bbt3fnxrTwS3R8FfAwzeWO0sgTlz9Rd7sweAAFkikV-q7czH65gYIpFolpPArJUsDDY5D4zupHTmngfJiTZAqiXjkNMY2gIGQm4SeS9Y9B4zRWGvedQW8aX-8ejMUGd-09okdcW6jI85XDIrxO_BdxZkM-vwBP4133LQEAAKqVmB1bu5Y1yo6-ZKtZzPNRqElLF3R7PAK_NhXGODxr'\n",
    "# First we can set the \"headers\" to a few different web browser codes \n",
    "headers = requests.utils.default_headers()\n",
    "ua = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
    "headers.update({\"User-Agent\": ua})\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "# What is in response?\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e673d0-b5cf-4a98-8d97-671287305938",
   "metadata": {},
   "source": [
    "There are a number of \"response codes\", generally anything in the 200s is good, and anything in the 400s or 500s is bad. \n",
    "\n",
    "Here is the full list of codes: [https://en.wikipedia.org/wiki/List_of_HTTP_status_codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "\n",
    "Once we have retrieved the HTML data, the next step is to parse it using Beautiful Soup. We can do this by creating a BeautifulSoup object and passing the HTML data as a parameter.\n",
    "\n",
    "The second parameter `'html.parser'` specifies the parser to use for parsing the HTML data. In this case, we are using the built-in HTML parser that comes with Beautiful Soup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9eca22-15ec-4b83-b66b-67b845a537fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bed07-0f28-487d-b718-fd29eb535ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What is in soup?\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b405aa-61b1-4b90-8e25-dda0695b8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = []\n",
    "for text in soup.find_all('strong', class_= 'listing-price d-block pull-left pr-25'):\n",
    "    prices.append(text.get_text(strip=True))\n",
    "\n",
    "beds = []\n",
    "for link in soup.find_all('strong', class_= 'info-item-value d-block pull-left pr-25'):\n",
    "    beds.append(link.get_text(strip=True))\n",
    "\n",
    "addresses = []\n",
    "for link in soup.find_all('h5', class_= 'card-title font-weight-bold listing-address mb-25'):\n",
    "    addresses.append(link.get_text(strip=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10103b-cd77-44a6-9639-75f42f59b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832031c-f4b5-46c3-ac62-3c5bbdf514f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
